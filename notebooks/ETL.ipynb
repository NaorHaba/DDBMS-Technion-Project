{"cells":[{"cell_type":"markdown","source":["#Extract Transform Load"],"metadata":{}},{"cell_type":"markdown","source":["This notebook will present the actions we executed to extract raw tweets from Kafka, transform and enrich (hydrate) them using Tweepy, and load them to Elasticsearch. <BR>\nFurther explanations about date selection, index creation and data transformations is presented in the attached PDF."],"metadata":{}},{"cell_type":"markdown","source":["##Imports"],"metadata":{}},{"cell_type":"code","source":["import tweepy\nfrom elasticsearch import Elasticsearch, helpers\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nimport pickle\nimport kafka\nfrom pyspark import SparkConf"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["##Twitter Api Configuration"],"metadata":{}},{"cell_type":"markdown","source":["We requested Twitter for a \"consumer key\" which allows us to hydrate the tweets we extract from Kafka."],"metadata":{}},{"cell_type":"code","source":["consumer_key = 'nT2y41biy35qNY8CsH95tBggi'\nconsumer_secret = 'b2axleaFed1GxDkSHU4dudPzmEez5fo3uZc6qvrUUwrLk5ttTE'\naccess_token = '1288512030165684224-CGyAOLR7MWKtL7qdze7qA9zvHwKlPw'\naccess_token_secret = 'Apnef4M0X0Y0PvuT5TT6c0kf7DjlqAVaSICpIawJ9pTFY'\nconsumer = kafka.KafkaConsumer(bootstrap_servers=[\"ddkafka.eastus.cloudapp.azure.com:9092\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["##Elasticsearch Configuration"],"metadata":{}},{"cell_type":"markdown","source":["Similar to what we've seen in the workshop, we create an Elasticsearch index only this time with extra settings to properly collect coordination data. <BR>\nTo maximize loading speed to the server, we used several notebooks. Each of them ETL different topics, thus the index name presented here. <BR>\nAs explained earlier, more information about that is presented in the PDF file."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n\nES_HOST = 'dds2019s-1002.eastus.cloudapp.azure.com'\nes = Elasticsearch([{'host': ES_HOST}], timeout=60000)\n\ndbutils.fs.rm(\"/tmp/Dvir/Stream/\", True)\ndbutils.fs.mkdirs(\"/tmp/Dvir/Stream/\")\n\nindex = 'trump_covid-19_tweets_11-03-2020_16-03-2020_2nd_try'\nif es.indices.exists(index): # Delete if exists\n  es.indices.delete(index=index)\n\nsettings = {\n  \"settings\" : {\n      \"number_of_shards\" : 1,\n      \"number_of_replicas\": 0,\n      \"refresh_interval\" : -1\n  },\n  \"mappings\" : {\n    \"properties\" : {\n      \"created_at\" : {\n        \"type\" : \"date\"\n      },\n      \"coordinates\" : {\n        \"properties\" : {\n          \"coordinates\" : {\n            \"type\" : \"geo_point\"\n          }\n        }\n      }\n    }\n  }\n}\nes.indices.create(index=index, ignore=400, body=settings)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: {&#39;acknowledged&#39;: True,\n &#39;shards_acknowledged&#39;: True,\n &#39;index&#39;: &#39;trump_covid-19_tweets_11-03-2020_16-03-2020_2nd_try&#39;}</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["##Extract From Kafka"],"metadata":{}},{"cell_type":"code","source":["raw_stream_df = spark.readStream \\\n                .format(\"kafka\") \\\n                .option(\"kafka.bootstrap.servers\", \"ddkafka.eastus.cloudapp.azure.com:9092\") \\\n                .option(\"subscribe\", \"11-03-2020, 16-03-2020\") \\\n                .option(\"startingOffsets\", \"earliest\") \\\n                .option(\"maxOffsetsPerTrigger\", \"100\")\\\n                .load()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["##Transform"],"metadata":{}},{"cell_type":"code","source":["schema = StructType() \\\n        .add(\"tweet_id\", LongType(), False) \\\n        .add(\"user_id\", LongType(), False) \\\n        .add(\"date\", StringType(), True) \\\n        .add(\"keywords\", ArrayType(StringType(), True), True) \\\n        .add(\"location\", MapType(StringType(), StringType(), True), True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["json_df = raw_stream_df.selectExpr(\"CAST(value AS STRING)\")\\\n                       .select(F.from_json(F.col(\"value\"), schema= schema).alias('json'))\\\n                       .select(\"json.*\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["##Hydrate tweets, transform them and load to Elasticsearch"],"metadata":{}},{"cell_type":"markdown","source":["Tuning the tweet schema to correctly receive the data we want."],"metadata":{}},{"cell_type":"code","source":["tweet_schema = pickle.load(open('/dbfs/mnt/tweet_schema.pkl', 'rb'))\nnew_schema = StructType()\nnew_schema.add(StructField(\"created_at\",DateType(),True))\nindex_list=[1, 4, 7, 22, 27, 28]\nfor i in index_list:\n  new_schema.add(tweet_schema[i])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["To hydrate the tweets, we used Tweepy api. Twitter restrict the amount of tweets that can be hydrated in a period of time so we collect the data in batches of 100 tweets at a time and work on each batch seperately. <BR>\nFor each batch we use the twitter id attribute, extracted from Kafka, in the \"statuses_lookup\" function which returns the hydrated tweets. <BR>\nNext we select and transform the data according to our plans and then load them to Elasticsearch."],"metadata":{}},{"cell_type":"code","source":["def foreach_batch_function(df, epoch_id):\n  # Transform and write batchDF\n  tweets_id_list = [str(tweet.tweet_id) for tweet in df.collect()]\n  tweets_list = api.statuses_lookup(id_=tweets_id_list, map=False)\n  df_tweets = sqlContext.createDataFrame(tweets_list, schema=new_schema)\n  df_tweets = df_tweets.select(F.col('created_at'), F.col('entities.hashtags.text').alias('hashtags'), 'coordinates', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count') \\\n                       .withColumn('created_at', F.to_timestamp(F.col('created_at'), \"EEE MMM dd HH:mm:ss ZZZZ yyyy\"))\n                       \n  df_tweets.write \\\n    .format(\"org.elasticsearch.spark.sql\") \\\n    .option(\"es.nodes.wan.only\", \"true\") \\\n    .option(\"es.resource\", index) \\\n    .option(\"es.nodes\", ES_HOST) \\\n    .option(\"es.port\",\"9200\") \\\n    .option(\"es.nodes.client.only\", \"false\") \\\n    .mode(\"append\") \\\n    .save()\n  pass"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["write_df = json_df.writeStream \\\n       .foreachBatch(foreach_batch_function) \\\n       .start()"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"ETL_206014185_206330342","notebookId":2007937552373081},"nbformat":4,"nbformat_minor":0}
